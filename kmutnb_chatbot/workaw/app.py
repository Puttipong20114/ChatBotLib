import os
import google.generativeai as genai
import pandas as pd
import streamlit as st
from prompt import PROMPT_WORKAW
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from document_reader import get_kmutnb_summary

# ‚úÖ ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ç‡∏≠: ‡πÑ‡∏°‡πà‡πÅ‡∏ï‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
genai.configure(api_key="AIzaSyD9ycgboJDlkj-JoyRJy8QKaAagEq3TAEQ")

# ----------------- CONFIG -----------------
generation_config = {
    "temperature": 0.1,
    "top_p": 0.95,
    "top_k": 64,
    # "max_output_tokens": 8192,
    "max_output_tokens": 512,  # ‡∏•‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô rate limit ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÑ‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    "response_mime_type": "text/plain",
    "candidate_count": 1,      # ‡πÉ‡∏´‡πâ‡∏£‡∏∏‡πà‡∏ô‡∏ï‡∏≠‡∏ö‡∏°‡∏≤‡∏ä‡πâ‡∏≠‡∏¢‡∏™‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
}

SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE
}

# ‡∏™‡∏£‡πâ‡∏≤‡∏á model ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
@st.cache_resource(show_spinner=False)
def get_model():
    return genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        safety_settings=SAFETY_SETTINGS,
        generation_config=generation_config,
        system_instruction=PROMPT_WORKAW
    )

model = get_model()

# ----------------- IO & CACHE -----------------
# ‡∏≠‡πà‡∏≤‡∏ô/‡∏™‡∏£‡∏∏‡∏õ PDF ‡πÅ‡∏•‡πâ‡∏ß cache ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡∏Å‡∏±‡∏ô I/O ‡∏´‡∏ô‡∏±‡∏Å ‡πÜ ‡∏ï‡∏≠‡∏ô rerun
@st.cache_data(show_spinner=True)
def load_kmutnb_summary(path: str) -> str:
    return get_kmutnb_summary(path)

# ----------------- UI -----------------
def clear_history():
    st.session_state["messages"] = [
        {"role": "model", "content": "KMUTNB Library Chatbot ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö KMUTNB Library ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏î‡∏Ñ‡∏£‡∏±‡∏ö"}
    ]
    # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡πÅ‡∏ä‡∏ó‡πÄ‡∏ã‡∏™‡∏ä‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏µ‡∏≠‡∏¥‡∏ô‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå context ‡πÉ‡∏´‡∏°‡πà
    st.session_state.pop("chat_session", None)
    st.rerun()

with st.sidebar:
    if st.button("Clear History"):
        clear_history()

st.title("üí¨ KMUTNB Library Chatbot ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {
            "role": "model",
            "content": "KMUTNB Library Chatbot ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö KMUTNB Library ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏î‡∏Ñ‡∏£‡∏±‡∏ö",
        }
    ]

# ----------------- LOAD DATASET ONCE -----------------
file_path = os.path.join(os.path.dirname(__file__), "DataSetLibraly.pdf")

try:
    file_content = load_kmutnb_summary(file_path)
    if isinstance(file_content, str) and file_content.startswith("Error:"):
        st.error(file_content)
        st.stop()
except Exception as e:
    st.error(f"Error reading file: {e}")
    st.stop()
# ----------------- CREATE / REUSE CHAT SESSION -----------------
# ‡∏¢‡∏±‡∏î context ‡∏à‡∏≤‡∏Å PDF ‡πÄ‡∏Ç‡πâ‡∏≤ history ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ session ‡πÄ‡∏î‡∏¥‡∏°‡∏ï‡πà‡∏≠ ‡∏¢‡πà‡∏ô‡πÄ‡∏ß‡∏•‡∏≤!
def ensure_chat_session():
    if "chat_session" not in st.session_state:
        # ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö history ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: system ‡πÄ‡∏õ‡∏¥‡∏î‡∏ö‡∏ó + user ‡πÉ‡∏™‡πà‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å PDF ‡πÄ‡∏õ‡πá‡∏ô context
        base_history = [
            {
                "role": "model",
                "parts": [{"text": "KMUTNB Library Chatbot ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏ö‡πÑ‡∏ß‡πâ"}],
            },
            {
                "role": "user",
                "parts": [{
                    "text": (
                        "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• KMUTN Library ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ "
                        "(‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡πà‡∏≠‡∏á‡∏à‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î):\n\n"
                        + file_content
                    )
                }],
            },
        ]
        st.session_state["chat_session"] = model.start_chat(history=base_history)

ensure_chat_session()

# ----------------- RENDER HISTORY (‡πÇ‡∏ä‡∏ß‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡πâ‡∏≤‡∏¢ ‡πÜ ‡πÉ‡∏´‡πâ‡πÑ‡∏ß) -----------------
def render_messages(limit_last:int = 20):
    for msg in st.session_state["messages"][-limit_last:]:
        st.chat_message(msg["role"]).write(msg["content"])

render_messages()

# ----------------- HANDLE INPUT -----------------
prompt = st.chat_input(placeholder="‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö KMUTNB Library ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‚ú®")

def trim_history(max_pairs:int = 8):
    """
    ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß history ‡πÉ‡∏ô UI (‡πÅ‡∏•‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ)
    ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏π‡πà‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡πâ‡∏≤‡∏¢ ‡πÜ ‡∏•‡∏î token ‚Üí ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    """
    msgs = st.session_state["messages"]
    # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡πâ‡∏≤‡∏¢ ‡πÜ
    if len(msgs) > (2 * max_pairs + 1):  # model msg ‡πÅ‡∏£‡∏Å + n*2 (user/model)
        st.session_state["messages"] = msgs[-(2 * max_pairs + 1):]

def generate_response(user_text: str):
    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï UI history
    st.session_state["messages"].append({"role": "user", "content": user_text})
    st.chat_message("user").write(user_text)

    # ‡πÇ‡∏´‡∏°‡∏î‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì
    if user_text.lower().startswith("add") or user_text.lower().endswith("add"):
        reply = "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ñ‡∏£‡∏±‡∏ö"
        st.session_state["messages"].append({"role": "model", "content": reply})
        st.chat_message("model").write(reply)
        trim_history()
        return

    # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏ï‡∏£‡∏µ‡∏°‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö (latency ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏ö‡∏ö non-stream)
    placeholder = st.chat_message("model")
    stream_box = placeholder.empty()
    collected = []

    try:
        for chunk in st.session_state["chat_session"].send_message(user_text, stream=True):
            piece = getattr(chunk, "text", None)
            if piece:
                collected.append(piece)
                stream_box.write("".join(collected))
        final_text = "".join(collected).strip() or "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ"
    except Exception as e:
        final_text = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö: {e}"

    st.session_state["messages"].append({"role": "model", "content": final_text})
    trim_history()

if prompt:
    generate_response(prompt)
